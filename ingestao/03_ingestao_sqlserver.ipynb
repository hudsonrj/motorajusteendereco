{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestão de Dados - SQL Server\n",
    "\n",
    "Este notebook realiza a ingestão de dados de um banco de dados SQL Server para o MinIO usando DeltaLake.\n",
    "\n",
    "## Configuração\n",
    "\n",
    "Configure as variáveis abaixo antes de executar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar configurações base\n",
    "%run ../00_configuracao_inicial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURAÇÕES DE CONEXÃO SQL SERVER\n",
    "# ============================================\n",
    "import os\n",
    "\n",
    "# Configurações de conexão SQL Server\n",
    "SQLSERVER_HOST = os.getenv('SQLSERVER_HOST', 'localhost')\n",
    "SQLSERVER_PORT = os.getenv('SQLSERVER_PORT', '1433')\n",
    "SQLSERVER_DATABASE = os.getenv('SQLSERVER_DATABASE', 'master')\n",
    "SQLSERVER_USER = os.getenv('SQLSERVER_USER', 'sa')\n",
    "SQLSERVER_PASSWORD = os.getenv('SQLSERVER_PASSWORD', 'senha')\n",
    "\n",
    "# Configurações de leitura\n",
    "SQLSERVER_SCHEMA = os.getenv('SQLSERVER_SCHEMA', 'dbo')\n",
    "SQLSERVER_TABLE = os.getenv('SQLSERVER_TABLE', 'nome_tabela')\n",
    "\n",
    "# Configurações de destino no MinIO\n",
    "DESTINO_BRONZE = f\"{PATH_BRONZE}/sqlserver/{SQLSERVER_DATABASE.lower()}/{SQLSERVER_SCHEMA.lower()}/{SQLSERVER_TABLE.lower()}\"\n",
    "\n",
    "print(\"Configurações SQL Server:\")\n",
    "print(f\"Host: {SQLSERVER_HOST}\")\n",
    "print(f\"Port: {SQLSERVER_PORT}\")\n",
    "print(f\"Database: {SQLSERVER_DATABASE}\")\n",
    "print(f\"Schema: {SQLSERVER_SCHEMA}\")\n",
    "print(f\"Table: {SQLSERVER_TABLE}\")\n",
    "print(f\"Destino: {DESTINO_BRONZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar driver SQL Server (executar apenas uma vez)\n",
    "# !pip install pymssql\n",
    "\n",
    "# Adicionar driver JDBC SQL Server ao Spark\n",
    "# Baixar mssql-jdbc-XX.X.X.jre8.jar e colocar no classpath do Spark\n",
    "# Ou usar: spark.jars.packages com coordenadas Maven: com.microsoft.sqlserver:mssql-jdbc:XX.X.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# URL de conexão JDBC SQL Server\n",
    "# Formato: jdbc:sqlserver://[serverName[\\instanceName][:portNumber]][;property=value[;property=value]]\n",
    "jdbc_url = f\"jdbc:sqlserver://{SQLSERVER_HOST}:{SQLSERVER_PORT};databaseName={SQLSERVER_DATABASE};encrypt=true;trustServerCertificate=true\"\n",
    "\n",
    "# Propriedades de conexão\n",
    "connection_properties = {\n",
    "    \"user\": SQLSERVER_USER,\n",
    "    \"password\": SQLSERVER_PASSWORD,\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "print(f\"JDBC URL: {jdbc_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para ler dados do SQL Server\n",
    "def ler_sqlserver_table(table_name, schema=\"dbo\", query=None, partition_column=None, num_partitions=None, lower_bound=None, upper_bound=None):\n",
    "    \"\"\"\n",
    "    Lê dados de uma tabela SQL Server\n",
    "    \n",
    "    Args:\n",
    "        table_name: Nome da tabela\n",
    "        schema: Schema (padrão: dbo)\n",
    "        query: Query SQL customizada (opcional, substitui table_name)\n",
    "        partition_column: Coluna para particionamento paralelo (opcional)\n",
    "        num_partitions: Número de partições (opcional)\n",
    "        lower_bound: Valor mínimo para particionamento (opcional)\n",
    "        upper_bound: Valor máximo para particionamento (opcional)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame do Spark\n",
    "    \"\"\"\n",
    "    if query:\n",
    "        # Usar query customizada (subquery)\n",
    "        table_or_query = f\"({query}) sqlserver_table\"\n",
    "    elif schema:\n",
    "        table_or_query = f\"{schema}.{table_name}\"\n",
    "    else:\n",
    "        table_or_query = table_name\n",
    "    \n",
    "    reader = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"dbtable\", table_or_query) \\\n",
    "        .option(\"user\", SQLSERVER_USER) \\\n",
    "        .option(\"password\", SQLSERVER_PASSWORD) \\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\n",
    "    \n",
    "    # Adicionar opções de particionamento se fornecidas\n",
    "    if partition_column and num_partitions:\n",
    "        reader = reader.option(\"partitionColumn\", partition_column) \\\n",
    "                      .option(\"numPartitions\", num_partitions)\n",
    "        if lower_bound is not None and upper_bound is not None:\n",
    "            reader = reader.option(\"lowerBound\", lower_bound) \\\n",
    "                          .option(\"upperBound\", upper_bound)\n",
    "    \n",
    "    df = reader.load()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 1: Leitura simples de tabela\n",
    "print(\"Exemplo 1: Leitura simples\")\n",
    "df_sqlserver = ler_sqlserver_table(\n",
    "    table_name=SQLSERVER_TABLE,\n",
    "    schema=SQLSERVER_SCHEMA\n",
    ")\n",
    "\n",
    "print(f\"Total de registros: {df_sqlserver.count()}\")\n",
    "df_sqlserver.printSchema()\n",
    "df_sqlserver.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 2: Leitura com query customizada\n",
    "print(\"Exemplo 2: Leitura com query customizada\")\n",
    "query_customizada = f\"\"\"\n",
    "    SELECT \n",
    "        coluna1,\n",
    "        coluna2,\n",
    "        coluna3,\n",
    "        UpdatedAt\n",
    "    FROM {SQLSERVER_SCHEMA}.{SQLSERVER_TABLE}\n",
    "    WHERE UpdatedAt >= DATEADD(day, -30, GETDATE())\n",
    "    ORDER BY UpdatedAt DESC\n",
    "\"\"\"\n",
    "\n",
    "# df_sqlserver_query = ler_sqlserver_table(query=query_customizada)\n",
    "# df_sqlserver_query.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 3: Leitura com particionamento paralelo (para tabelas grandes)\n",
    "print(\"Exemplo 3: Leitura com particionamento\")\n",
    "# df_sqlserver_partitioned = ler_sqlserver_table(\n",
    "#     table_name=SQLSERVER_TABLE,\n",
    "#     schema=SQLSERVER_SCHEMA,\n",
    "#     partition_column=\"Id\",  # Coluna numérica para particionamento\n",
    "#     num_partitions=10,\n",
    "#     lower_bound=1,\n",
    "#     upper_bound=1000000\n",
    "# )\n",
    "# df_sqlserver_partitioned.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar metadados de ingestão\n",
    "df_ingestao = df_sqlserver \\\n",
    "    .withColumn(\"fonte\", lit(\"SQLSERVER\")) \\\n",
    "    .withColumn(\"database_origem\", lit(SQLSERVER_DATABASE)) \\\n",
    "    .withColumn(\"schema_origem\", lit(SQLSERVER_SCHEMA)) \\\n",
    "    .withColumn(\"tabela_origem\", lit(SQLSERVER_TABLE)) \\\n",
    "    .withColumn(\"ingestao_em\", current_timestamp()) \\\n",
    "    .withColumn(\"particao_data\", date_format(current_date(), \"yyyy-MM-dd\"))\n",
    "\n",
    "print(\"Metadados adicionados:\")\n",
    "df_ingestao.select(\"fonte\", \"database_origem\", \"schema_origem\", \"tabela_origem\", \"ingestao_em\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar no MinIO como Delta Table\n",
    "print(f\"Salvando dados em: {DESTINO_BRONZE}\")\n",
    "\n",
    "# save_delta_table(\n",
    "#     df_ingestao,\n",
    "#     DESTINO_BRONZE,\n",
    "#     mode=\"overwrite\",  # ou \"append\" para incrementais\n",
    "#     partition_by=[\"particao_data\"]  # Particionar por data\n",
    "# )\n",
    "\n",
    "print(\"Ingestão concluída com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar dados salvos\n",
    "# df_verificacao = read_delta_table(DESTINO_BRONZE)\n",
    "# print(f\"Registros salvos: {df_verificacao.count()}\")\n",
    "# df_verificacao.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestão Incremental\n",
    "\n",
    "Para ingestões incrementais baseadas em timestamp ou ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para ingestão incremental\n",
    "def ingestao_incremental_sqlserver(table_name, schema, coluna_timestamp=\"UpdatedAt\", ultima_execucao=None):\n",
    "    \"\"\"\n",
    "    Realiza ingestão incremental de dados SQL Server\n",
    "    \n",
    "    Args:\n",
    "        table_name: Nome da tabela\n",
    "        schema: Schema\n",
    "        coluna_timestamp: Nome da coluna de timestamp para filtro\n",
    "        ultima_execucao: Timestamp da última execução (formato: 'YYYY-MM-DD HH24:MI:SS')\n",
    "    \"\"\"\n",
    "    if ultima_execucao:\n",
    "        query = f\"\"\"\n",
    "            SELECT * FROM {schema}.{table_name}\n",
    "            WHERE {coluna_timestamp} > '{ultima_execucao}'\n",
    "            ORDER BY {coluna_timestamp}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        # Primeira execução: pegar últimos 7 dias\n",
    "        query = f\"\"\"\n",
    "            SELECT * FROM {schema}.{table_name}\n",
    "            WHERE {coluna_timestamp} >= DATEADD(day, -7, GETDATE())\n",
    "            ORDER BY {coluna_timestamp}\n",
    "        \"\"\"\n",
    "    \n",
    "    df_incremental = ler_sqlserver_table(query=query)\n",
    "    \n",
    "    return df_incremental\n",
    "\n",
    "# Exemplo de uso\n",
    "# df_incremental = ingestao_incremental_sqlserver(\n",
    "#     table_name=SQLSERVER_TABLE,\n",
    "#     schema=SQLSERVER_SCHEMA,\n",
    "#     ultima_execucao=\"2024-01-01 00:00:00\"\n",
    "# )\n",
    "# \n",
    "# # Salvar em modo append\n",
    "# save_delta_table(df_incremental, DESTINO_BRONZE, mode=\"append\", partition_by=[\"particao_data\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
