{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplos de Ingestão - Todos os Bancos de Dados\n",
    "\n",
    "Este notebook contém exemplos rápidos de ingestão para todos os bancos de dados suportados.\n",
    "\n",
    "## Bancos Suportados\n",
    "\n",
    "1. **Oracle** - Ver notebook: `01_ingestao_oracle.ipynb`\n",
    "2. **PostgreSQL** - Ver notebook: `02_ingestao_postgresql.ipynb`\n",
    "3. **SQL Server** - Ver notebook: `03_ingestao_sqlserver.ipynb`\n",
    "4. **MySQL** - Ver notebook: `04_ingestao_mysql.ipynb`\n",
    "5. **MongoDB** - Ver notebook: `05_ingestao_mongodb.ipynb`\n",
    "\n",
    "## Uso Rápido\n",
    "\n",
    "Este notebook serve como referência rápida. Para implementações completas, use os notebooks específicos de cada banco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar configurações base\n",
    "%run ../00_configuracao_inicial.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Oracle Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo Oracle\n",
    "# from pyspark.sql.functions import *\n",
    "# \n",
    "# jdbc_url = \"jdbc:oracle:thin:@localhost:1521/ORCL\"\n",
    "# \n",
    "# df_oracle = spark.read.format(\"jdbc\") \\\n",
    "#     .option(\"url\", jdbc_url) \\\n",
    "#     .option(\"dbtable\", \"SCHEMA.TABELA\") \\\n",
    "#     .option(\"user\", \"usuario\") \\\n",
    "#     .option(\"password\", \"senha\") \\\n",
    "#     .option(\"driver\", \"oracle.jdbc.OracleDriver\") \\\n",
    "#     .load()\n",
    "# \n",
    "# # Adicionar metadados e salvar\n",
    "# df_oracle = df_oracle.withColumn(\"fonte\", lit(\"ORACLE\")) \\\n",
    "#                      .withColumn(\"ingestao_em\", current_timestamp())\n",
    "# \n",
    "# save_delta_table(df_oracle, f\"{PATH_BRONZE}/oracle/tabela\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo PostgreSQL\n",
    "# from pyspark.sql.functions import *\n",
    "# \n",
    "# jdbc_url = \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "# \n",
    "# df_postgres = spark.read.format(\"jdbc\") \\\n",
    "#     .option(\"url\", jdbc_url) \\\n",
    "#     .option(\"dbtable\", \"public.tabela\") \\\n",
    "#     .option(\"user\", \"postgres\") \\\n",
    "#     .option(\"password\", \"senha\") \\\n",
    "#     .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "#     .load()\n",
    "# \n",
    "# # Adicionar metadados e salvar\n",
    "# df_postgres = df_postgres.withColumn(\"fonte\", lit(\"POSTGRESQL\")) \\\n",
    "#                          .withColumn(\"ingestao_em\", current_timestamp())\n",
    "# \n",
    "# save_delta_table(df_postgres, f\"{PATH_BRONZE}/postgresql/tabela\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo SQL Server\n",
    "# from pyspark.sql.functions import *\n",
    "# \n",
    "# jdbc_url = \"jdbc:sqlserver://localhost:1433;databaseName=master;encrypt=true;trustServerCertificate=true\"\n",
    "# \n",
    "# df_sqlserver = spark.read.format(\"jdbc\") \\\n",
    "#     .option(\"url\", jdbc_url) \\\n",
    "#     .option(\"dbtable\", \"dbo.tabela\") \\\n",
    "#     .option(\"user\", \"sa\") \\\n",
    "#     .option(\"password\", \"senha\") \\\n",
    "#     .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "#     .load()\n",
    "# \n",
    "# # Adicionar metadados e salvar\n",
    "# df_sqlserver = df_sqlserver.withColumn(\"fonte\", lit(\"SQLSERVER\")) \\\n",
    "#                             .withColumn(\"ingestao_em\", current_timestamp())\n",
    "# \n",
    "# save_delta_table(df_sqlserver, f\"{PATH_BRONZE}/sqlserver/tabela\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo MySQL\n",
    "# from pyspark.sql.functions import *\n",
    "# \n",
    "# jdbc_url = \"jdbc:mysql://localhost:3306/mysql?useSSL=false&serverTimezone=UTC\"\n",
    "# \n",
    "# df_mysql = spark.read.format(\"jdbc\") \\\n",
    "#     .option(\"url\", jdbc_url) \\\n",
    "#     .option(\"dbtable\", \"tabela\") \\\n",
    "#     .option(\"user\", \"root\") \\\n",
    "#     .option(\"password\", \"senha\") \\\n",
    "#     .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "#     .load()\n",
    "# \n",
    "# # Adicionar metadados e salvar\n",
    "# df_mysql = df_mysql.withColumn(\"fonte\", lit(\"MYSQL\")) \\\n",
    "#                    .withColumn(\"ingestao_em\", current_timestamp())\n",
    "# \n",
    "# save_delta_table(df_mysql, f\"{PATH_BRONZE}/mysql/tabela\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo MongoDB\n",
    "# from pyspark.sql.functions import *\n",
    "# \n",
    "# mongodb_uri = \"mongodb://admin:senha@localhost:27017/admin?authSource=admin\"\n",
    "# \n",
    "# df_mongodb = spark.read.format(\"mongo\") \\\n",
    "#     .option(\"uri\", mongodb_uri) \\\n",
    "#     .option(\"database\", \"admin\") \\\n",
    "#     .option(\"collection\", \"colecao\") \\\n",
    "#     .load()\n",
    "# \n",
    "# # Adicionar metadados e salvar\n",
    "# df_mongodb = df_mongodb.withColumn(\"fonte\", lit(\"MONGODB\")) \\\n",
    "#                         .withColumn(\"ingestao_em\", current_timestamp())\n",
    "# \n",
    "# save_delta_table(df_mongodb, f\"{PATH_BRONZE}/mongodb/colecao\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função Genérica de Ingestão\n",
    "\n",
    "Função auxiliar para facilitar a ingestão de qualquer banco JDBC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "def ingestao_jdbc_generica(\n",
    "    jdbc_url,\n",
    "    driver,\n",
    "    table_or_query,\n",
    "    user,\n",
    "    password,\n",
    "    destino_path,\n",
    "    fonte_nome,\n",
    "    mode=\"overwrite\",\n",
    "    partition_by=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Função genérica para ingestão de qualquer banco JDBC\n",
    "    \n",
    "    Args:\n",
    "        jdbc_url: URL JDBC de conexão\n",
    "        driver: Nome da classe do driver JDBC\n",
    "        table_or_query: Nome da tabela ou query SQL\n",
    "        user: Usuário\n",
    "        password: Senha\n",
    "        destino_path: Caminho de destino no MinIO\n",
    "        fonte_nome: Nome da fonte (para metadados)\n",
    "        mode: Modo de escrita (overwrite, append)\n",
    "        partition_by: Lista de colunas para particionamento\n",
    "    \"\"\"\n",
    "    # Ler dados\n",
    "    df = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"dbtable\", table_or_query) \\\n",
    "        .option(\"user\", user) \\\n",
    "        .option(\"password\", password) \\\n",
    "        .option(\"driver\", driver) \\\n",
    "        .load()\n",
    "    \n",
    "    # Adicionar metadados\n",
    "    df_ingestao = df \\\n",
    "        .withColumn(\"fonte\", lit(fonte_nome)) \\\n",
    "        .withColumn(\"ingestao_em\", current_timestamp()) \\\n",
    "        .withColumn(\"particao_data\", date_format(current_date(), \"yyyy-MM-dd\"))\n",
    "    \n",
    "    # Salvar\n",
    "    save_delta_table(df_ingestao, destino_path, mode=mode, partition_by=partition_by)\n",
    "    \n",
    "    return df_ingestao\n",
    "\n",
    "# Exemplo de uso\n",
    "# df = ingestao_jdbc_generica(\n",
    "#     jdbc_url=\"jdbc:postgresql://localhost:5432/postgres\",\n",
    "#     driver=\"org.postgresql.Driver\",\n",
    "#     table_or_query=\"public.tabela\",\n",
    "#     user=\"postgres\",\n",
    "#     password=\"senha\",\n",
    "#     destino_path=f\"{PATH_BRONZE}/postgresql/tabela\",\n",
    "#     fonte_nome=\"POSTGRESQL\",\n",
    "#     mode=\"overwrite\",\n",
    "#     partition_by=[\"particao_data\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checklist de Configuração\n",
    "\n",
    "Antes de executar qualquer ingestão, verifique:\n",
    "\n",
    "1. ✅ Driver JDBC instalado/configurado no Spark\n",
    "2. ✅ Credenciais de acesso configuradas\n",
    "3. ✅ Conectividade de rede testada\n",
    "4. ✅ Permissões de leitura no banco de origem\n",
    "5. ✅ Permissões de escrita no MinIO\n",
    "6. ✅ Caminho de destino configurado\n",
    "\n",
    "## Drivers JDBC Necessários\n",
    "\n",
    "- **Oracle**: `ojdbc8.jar` ou `oracle.jdbc.OracleDriver`\n",
    "- **PostgreSQL**: `postgresql-XX.X.jar` (já incluído no Spark)\n",
    "- **SQL Server**: `mssql-jdbc-XX.X.X.jre8.jar`\n",
    "- **MySQL**: `mysql-connector-java-XX.X.jar` (já incluído no Spark)\n",
    "- **MongoDB**: `mongo-spark-connector_2.12-XX.X.X.jar`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
