{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestão de Dados - PostgreSQL\n",
    "\n",
    "Este notebook realiza a ingestão de dados de um banco de dados PostgreSQL para o MinIO usando DeltaLake.\n",
    "\n",
    "## Configuração\n",
    "\n",
    "Configure as variáveis abaixo antes de executar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar configurações base\n",
    "%run ../00_configuracao_inicial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURAÇÕES DE CONEXÃO POSTGRESQL\n",
    "# ============================================\n",
    "import os\n",
    "\n",
    "# Configurações de conexão PostgreSQL\n",
    "POSTGRES_HOST = os.getenv('POSTGRES_HOST', 'localhost')\n",
    "POSTGRES_PORT = os.getenv('POSTGRES_PORT', '5432')\n",
    "POSTGRES_DATABASE = os.getenv('POSTGRES_DATABASE', 'postgres')\n",
    "POSTGRES_USER = os.getenv('POSTGRES_USER', 'postgres')\n",
    "POSTGRES_PASSWORD = os.getenv('POSTGRES_PASSWORD', 'senha')\n",
    "\n",
    "# Configurações de leitura\n",
    "POSTGRES_SCHEMA = os.getenv('POSTGRES_SCHEMA', 'public')\n",
    "POSTGRES_TABLE = os.getenv('POSTGRES_TABLE', 'nome_tabela')\n",
    "\n",
    "# Configurações de destino no MinIO\n",
    "DESTINO_BRONZE = f\"{PATH_BRONZE}/postgresql/{POSTGRES_DATABASE.lower()}/{POSTGRES_SCHEMA.lower()}/{POSTGRES_TABLE.lower()}\"\n",
    "\n",
    "print(\"Configurações PostgreSQL:\")\n",
    "print(f\"Host: {POSTGRES_HOST}\")\n",
    "print(f\"Port: {POSTGRES_PORT}\")\n",
    "print(f\"Database: {POSTGRES_DATABASE}\")\n",
    "print(f\"Schema: {POSTGRES_SCHEMA}\")\n",
    "print(f\"Table: {POSTGRES_TABLE}\")\n",
    "print(f\"Destino: {DESTINO_BRONZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar driver PostgreSQL (executar apenas uma vez)\n",
    "# !pip install psycopg2-binary\n",
    "\n",
    "# O Spark já inclui o driver JDBC PostgreSQL por padrão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# URL de conexão JDBC PostgreSQL\n",
    "jdbc_url = f\"jdbc:postgresql://{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DATABASE}\"\n",
    "\n",
    "# Propriedades de conexão\n",
    "connection_properties = {\n",
    "    \"user\": POSTGRES_USER,\n",
    "    \"password\": POSTGRES_PASSWORD,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "print(f\"JDBC URL: {jdbc_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para ler dados do PostgreSQL\n",
    "def ler_postgresql_table(table_name, schema=\"public\", query=None, partition_column=None, num_partitions=None, lower_bound=None, upper_bound=None):\n",
    "    \"\"\"\n",
    "    Lê dados de uma tabela PostgreSQL\n",
    "    \n",
    "    Args:\n",
    "        table_name: Nome da tabela\n",
    "        schema: Schema (padrão: public)\n",
    "        query: Query SQL customizada (opcional, substitui table_name)\n",
    "        partition_column: Coluna para particionamento paralelo (opcional)\n",
    "        num_partitions: Número de partições (opcional)\n",
    "        lower_bound: Valor mínimo para particionamento (opcional)\n",
    "        upper_bound: Valor máximo para particionamento (opcional)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame do Spark\n",
    "    \"\"\"\n",
    "    if query:\n",
    "        # Usar query customizada (subquery)\n",
    "        table_or_query = f\"({query}) postgres_table\"\n",
    "    elif schema:\n",
    "        table_or_query = f\"{schema}.{table_name}\"\n",
    "    else:\n",
    "        table_or_query = table_name\n",
    "    \n",
    "    reader = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"dbtable\", table_or_query) \\\n",
    "        .option(\"user\", POSTGRES_USER) \\\n",
    "        .option(\"password\", POSTGRES_PASSWORD) \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\")\n",
    "    \n",
    "    # Adicionar opções de particionamento se fornecidas\n",
    "    if partition_column and num_partitions:\n",
    "        reader = reader.option(\"partitionColumn\", partition_column) \\\n",
    "                      .option(\"numPartitions\", num_partitions)\n",
    "        if lower_bound is not None and upper_bound is not None:\n",
    "            reader = reader.option(\"lowerBound\", lower_bound) \\\n",
    "                          .option(\"upperBound\", upper_bound)\n",
    "    \n",
    "    df = reader.load()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 1: Leitura simples de tabela\n",
    "print(\"Exemplo 1: Leitura simples\")\n",
    "df_postgres = ler_postgresql_table(\n",
    "    table_name=POSTGRES_TABLE,\n",
    "    schema=POSTGRES_SCHEMA\n",
    ")\n",
    "\n",
    "print(f\"Total de registros: {df_postgres.count()}\")\n",
    "df_postgres.printSchema()\n",
    "df_postgres.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 2: Leitura com query customizada\n",
    "print(\"Exemplo 2: Leitura com query customizada\")\n",
    "query_customizada = f\"\"\"\n",
    "    SELECT \n",
    "        coluna1,\n",
    "        coluna2,\n",
    "        coluna3,\n",
    "        updated_at\n",
    "    FROM {POSTGRES_SCHEMA}.{POSTGRES_TABLE}\n",
    "    WHERE updated_at >= NOW() - INTERVAL '30 days'\n",
    "    ORDER BY updated_at DESC\n",
    "\"\"\"\n",
    "\n",
    "# df_postgres_query = ler_postgresql_table(query=query_customizada)\n",
    "# df_postgres_query.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 3: Leitura com particionamento paralelo (para tabelas grandes)\n",
    "print(\"Exemplo 3: Leitura com particionamento\")\n",
    "# df_postgres_partitioned = ler_postgresql_table(\n",
    "#     table_name=POSTGRES_TABLE,\n",
    "#     schema=POSTGRES_SCHEMA,\n",
    "#     partition_column=\"id\",  # Coluna numérica para particionamento\n",
    "#     num_partitions=10,\n",
    "#     lower_bound=1,\n",
    "#     upper_bound=1000000\n",
    "# )\n",
    "# df_postgres_partitioned.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar metadados de ingestão\n",
    "df_ingestao = df_postgres \\\n",
    "    .withColumn(\"fonte\", lit(\"POSTGRESQL\")) \\\n",
    "    .withColumn(\"database_origem\", lit(POSTGRES_DATABASE)) \\\n",
    "    .withColumn(\"schema_origem\", lit(POSTGRES_SCHEMA)) \\\n",
    "    .withColumn(\"tabela_origem\", lit(POSTGRES_TABLE)) \\\n",
    "    .withColumn(\"ingestao_em\", current_timestamp()) \\\n",
    "    .withColumn(\"particao_data\", date_format(current_date(), \"yyyy-MM-dd\"))\n",
    "\n",
    "print(\"Metadados adicionados:\")\n",
    "df_ingestao.select(\"fonte\", \"database_origem\", \"schema_origem\", \"tabela_origem\", \"ingestao_em\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar no MinIO como Delta Table\n",
    "print(f\"Salvando dados em: {DESTINO_BRONZE}\")\n",
    "\n",
    "# save_delta_table(\n",
    "#     df_ingestao,\n",
    "#     DESTINO_BRONZE,\n",
    "#     mode=\"overwrite\",  # ou \"append\" para incrementais\n",
    "#     partition_by=[\"particao_data\"]  # Particionar por data\n",
    "# )\n",
    "\n",
    "print(\"Ingestão concluída com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar dados salvos\n",
    "# df_verificacao = read_delta_table(DESTINO_BRONZE)\n",
    "# print(f\"Registros salvos: {df_verificacao.count()}\")\n",
    "# df_verificacao.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestão Incremental\n",
    "\n",
    "Para ingestões incrementais baseadas em timestamp ou ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para ingestão incremental\n",
    "def ingestao_incremental_postgresql(table_name, schema, coluna_timestamp=\"updated_at\", ultima_execucao=None):\n",
    "    \"\"\"\n",
    "    Realiza ingestão incremental de dados PostgreSQL\n",
    "    \n",
    "    Args:\n",
    "        table_name: Nome da tabela\n",
    "        schema: Schema\n",
    "        coluna_timestamp: Nome da coluna de timestamp para filtro\n",
    "        ultima_execucao: Timestamp da última execução (formato: 'YYYY-MM-DD HH24:MI:SS')\n",
    "    \"\"\"\n",
    "    if ultima_execucao:\n",
    "        query = f\"\"\"\n",
    "            SELECT * FROM {schema}.{table_name}\n",
    "            WHERE {coluna_timestamp} > '{ultima_execucao}'::timestamp\n",
    "            ORDER BY {coluna_timestamp}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        # Primeira execução: pegar últimos 7 dias\n",
    "        query = f\"\"\"\n",
    "            SELECT * FROM {schema}.{table_name}\n",
    "            WHERE {coluna_timestamp} >= NOW() - INTERVAL '7 days'\n",
    "            ORDER BY {coluna_timestamp}\n",
    "        \"\"\"\n",
    "    \n",
    "    df_incremental = ler_postgresql_table(query=query)\n",
    "    \n",
    "    return df_incremental\n",
    "\n",
    "# Exemplo de uso\n",
    "# df_incremental = ingestao_incremental_postgresql(\n",
    "#     table_name=POSTGRES_TABLE,\n",
    "#     schema=POSTGRES_SCHEMA,\n",
    "#     ultima_execucao=\"2024-01-01 00:00:00\"\n",
    "# )\n",
    "# \n",
    "# # Salvar em modo append\n",
    "# save_delta_table(df_incremental, DESTINO_BRONZE, mode=\"append\", partition_by=[\"particao_data\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
