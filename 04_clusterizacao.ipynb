{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusterização de Endereços\n",
    "\n",
    "Este notebook agrupa variações de um mesmo endereço sob um único UID (ID Único de Endereço), mesmo quando há diferenças na grafia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar configurações\n",
    "%run ./00_configuracao_inicial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import Tokenizer, NGram\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import hashlib\n",
    "\n",
    "# Importar funções de similaridade (será criado em notebook separado)\n",
    "# %run ./05_algoritmos_similaridade.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF para calcular similaridade Jaro-Winkler (simplificado)\n",
    "def jaro_winkler_similarity(s1, s2):\n",
    "    \"\"\"Calcula similaridade Jaro-Winkler entre duas strings\"\"\"\n",
    "    if not s1 or not s2:\n",
    "        return 0.0\n",
    "    \n",
    "    s1 = s1.upper().strip()\n",
    "    s2 = s2.upper().strip()\n",
    "    \n",
    "    if s1 == s2:\n",
    "        return 1.0\n",
    "    \n",
    "    # Implementação simplificada do Jaro-Winkler\n",
    "    # Para produção, usar biblioteca como jellyfish ou similar\n",
    "    len_s1 = len(s1)\n",
    "    len_s2 = len(s2)\n",
    "    \n",
    "    match_window = max(len_s1, len_s2) // 2 - 1\n",
    "    if match_window < 0:\n",
    "        match_window = 0\n",
    "    \n",
    "    s1_matches = [False] * len_s1\n",
    "    s2_matches = [False] * len_s2\n",
    "    \n",
    "    matches = 0\n",
    "    transpositions = 0\n",
    "    \n",
    "    # Encontrar matches\n",
    "    for i in range(len_s1):\n",
    "        start = max(0, i - match_window)\n",
    "        end = min(i + match_window + 1, len_s2)\n",
    "        \n",
    "        for j in range(start, end):\n",
    "            if s2_matches[j] or s1[i] != s2[j]:\n",
    "                continue\n",
    "            s1_matches[i] = True\n",
    "            s2_matches[j] = True\n",
    "            matches += 1\n",
    "            break\n",
    "    \n",
    "    if matches == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Contar transposições\n",
    "    k = 0\n",
    "    for i in range(len_s1):\n",
    "        if not s1_matches[i]:\n",
    "            continue\n",
    "        while not s2_matches[k]:\n",
    "            k += 1\n",
    "        if s1[i] != s2[k]:\n",
    "            transpositions += 1\n",
    "        k += 1\n",
    "    \n",
    "    jaro = (matches / len_s1 + matches / len_s2 + (matches - transpositions / 2) / matches) / 3.0\n",
    "    \n",
    "    # Winkler prefix bonus\n",
    "    prefix = 0\n",
    "    for i in range(min(len(s1), len(s2), 4)):\n",
    "        if s1[i] == s2[i]:\n",
    "            prefix += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    winkler = jaro + (0.1 * prefix * (1 - jaro))\n",
    "    \n",
    "    return min(1.0, winkler)\n",
    "\n",
    "udf_jaro_winkler = udf(jaro_winkler_similarity, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para agrupar endereços similares por UF+Cidade\n",
    "def clusterizar_enderecos(df_camada_ouro, threshold_similaridade=0.85):\n",
    "    \"\"\"\n",
    "    Agrupa endereços similares em clusters baseado em similaridade de string\n",
    "    \n",
    "    Args:\n",
    "        df_camada_ouro: DataFrame com Camada Ouro\n",
    "        threshold_similaridade: Threshold mínimo de similaridade para agrupar (0-1)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame com clusters de endereços\n",
    "    \"\"\"\n",
    "    # Criar chave de comparação (UF + Cidade + Tipo + Nome Logradouro)\n",
    "    df_com_chave = df_camada_ouro \\\n",
    "        .withColumn(\"chave_comparacao\", \n",
    "            concat_ws(\"|\", \n",
    "                col(\"uf\"),\n",
    "                col(\"cidade\"),\n",
    "                col(\"tipo_logradouro\"),\n",
    "                col(\"nome_logradouro\")\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Para cada registro, encontrar matches similares\n",
    "    # Usar self-join com condição de similaridade\n",
    "    df_renamed = df_com_chave.alias(\"a\")\n",
    "    df_compare = df_com_chave.alias(\"b\")\n",
    "    \n",
    "    # Join apenas em mesma UF+Cidade para reduzir comparações\n",
    "    df_similar = df_renamed.join(\n",
    "        df_compare,\n",
    "        (col(\"a.uf\") == col(\"b.uf\")) & \n",
    "        (col(\"a.cidade\") == col(\"b.cidade\")) &\n",
    "        (col(\"a.uid\") != col(\"b.uid\")),\n",
    "        \"inner\"\n",
    "    ) \\\n",
    "    .withColumn(\"similaridade\", \n",
    "        udf_jaro_winkler(col(\"a.nome_logradouro\"), col(\"b.nome_logradouro\"))\n",
    "    ) \\\n",
    "    .filter(col(\"similaridade\") >= threshold_similaridade) \\\n",
    "    .select(\n",
    "        col(\"a.uid\").alias(\"uid_origem\"),\n",
    "        col(\"b.uid\").alias(\"uid_destino\"),\n",
    "        col(\"similaridade\")\n",
    "    )\n",
    "    \n",
    "    return df_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para criar clusters conectados (usando grafos)\n",
    "def criar_clusters_conectados(df_similar):\n",
    "    \"\"\"\n",
    "    Cria clusters de endereços conectados por similaridade\n",
    "    Usa algoritmo de componentes conectados\n",
    "    \n",
    "    Args:\n",
    "        df_similar: DataFrame com pares de UIDs similares\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame com cluster_id para cada UID\n",
    "    \"\"\"\n",
    "    from graphframes import GraphFrame\n",
    "    \n",
    "    # Criar vértices (todos os UIDs únicos)\n",
    "    vertices = df_similar.select(\"uid_origem\").union(df_similar.select(\"uid_destino\")) \\\n",
    "        .distinct() \\\n",
    "        .withColumnRenamed(\"uid_origem\", \"id\")\n",
    "    \n",
    "    # Criar arestas (relações de similaridade)\n",
    "    edges = df_similar.select(\n",
    "        col(\"uid_origem\").alias(\"src\"),\n",
    "        col(\"uid_destino\").alias(\"dst\"),\n",
    "        col(\"similaridade\").alias(\"weight\")\n",
    "    )\n",
    "    \n",
    "    # Criar GraphFrame\n",
    "    graph = GraphFrame(vertices, edges)\n",
    "    \n",
    "    # Encontrar componentes conectados\n",
    "    clusters = graph.connectedComponents()\n",
    "    \n",
    "    # Renomear coluna de cluster\n",
    "    df_clusters = clusters.withColumnRenamed(\"component\", \"cluster_id\")\n",
    "    \n",
    "    return df_clusters\n",
    "\n",
    "# Alternativa sem GraphFrames (usando algoritmo simples)\n",
    "def criar_clusters_simples(df_similar, df_camada_ouro):\n",
    "    \"\"\"\n",
    "    Versão simplificada sem GraphFrames\n",
    "    Agrupa por maior similaridade\n",
    "    \"\"\"\n",
    "    from pyspark.sql.window import Window\n",
    "    \n",
    "    # Para cada UID, encontrar o melhor match\n",
    "    window_spec = Window.partitionBy(\"uid_origem\").orderBy(desc(\"similaridade\"))\n",
    "    \n",
    "    df_best_match = df_similar \\\n",
    "        .withColumn(\"rank\", row_number().over(window_spec)) \\\n",
    "        .filter(col(\"rank\") == 1) \\\n",
    "        .select(\n",
    "            col(\"uid_origem\").alias(\"uid\"),\n",
    "            col(\"uid_destino\").alias(\"cluster_representante\")\n",
    "        )\n",
    "    \n",
    "    # Criar cluster_id baseado no representante\n",
    "    df_clusters = df_camada_ouro.join(\n",
    "        df_best_match,\n",
    "        \"uid\",\n",
    "        \"left\"\n",
    "    ) \\\n",
    "    .withColumn(\"cluster_id\", \n",
    "        coalesce(col(\"cluster_representante\"), col(\"uid\"))  # Se não tem match, cluster = próprio UID\n",
    "    ) \\\n",
    "    .select(\"uid\", \"cluster_id\")\n",
    "    \n",
    "    return df_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função principal de clusterização\n",
    "def executar_clusterizacao(threshold_similaridade=0.85):\n",
    "    \"\"\"\n",
    "    Executa processo completo de clusterização\n",
    "    \n",
    "    Args:\n",
    "        threshold_similaridade: Threshold de similaridade para agrupar\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame com clusters\n",
    "    \"\"\"\n",
    "    print(\"1. Carregando Camada Ouro...\")\n",
    "    df_camada_ouro = read_delta_table(PATH_CAMADA_OURO)\n",
    "    print(f\"   Total de registros: {df_camada_ouro.count()}\")\n",
    "    \n",
    "    print(\"2. Identificando endereços similares...\")\n",
    "    df_similar = clusterizar_enderecos(df_camada_ouro, threshold_similaridade)\n",
    "    print(f\"   Total de pares similares: {df_similar.count()}\")\n",
    "    \n",
    "    print(\"3. Criando clusters...\")\n",
    "    df_clusters = criar_clusters_simples(df_similar, df_camada_ouro)\n",
    "    print(f\"   Total de clusters: {df_clusters.select('cluster_id').distinct().count()}\")\n",
    "    \n",
    "    # Adicionar informações da Camada Ouro aos clusters\n",
    "    df_clusters_completo = df_clusters.join(df_camada_ouro, \"uid\", \"inner\") \\\n",
    "        .withColumn(\"clusterizado_em\", current_timestamp())\n",
    "    \n",
    "    return df_clusters_completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar clusterização\n",
    "# df_clusters = executar_clusterizacao(threshold_similaridade=0.85)\n",
    "\n",
    "# Visualizar amostra\n",
    "# df_clusters.show(20, truncate=False)\n",
    "\n",
    "# Estatísticas de clusters\n",
    "# df_clusters.groupBy(\"cluster_id\").agg(\n",
    "#     count(\"*\").alias(\"tamanho_cluster\")\n",
    "# ).agg(\n",
    "#     avg(\"tamanho_cluster\").alias(\"tamanho_medio\"),\n",
    "#     max(\"tamanho_cluster\").alias(\"tamanho_max\")\n",
    "# ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar clusters\n",
    "# save_delta_table(df_clusters, PATH_CLUSTERS, mode=\"overwrite\", partition_by=[\"uf\", \"cidade\"])\n",
    "\n",
    "print(\"Clusterização concluída!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
