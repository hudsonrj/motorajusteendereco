{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuração Inicial - Spark, DeltaLake e MinIO\n",
    "\n",
    "Este notebook configura o ambiente PySpark com DeltaLake e conexão ao MinIO para armazenamento de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação de dependências (executar apenas uma vez)\n",
    "# !pip install pyspark delta-spark minio boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta import configure_spark_with_delta_pip\n",
    "import os\n",
    "\n",
    "# Configurações do MinIO\n",
    "MINIO_ENDPOINT = os.getenv('MINIO_ENDPOINT', 'localhost:9000')\n",
    "MINIO_ACCESS_KEY = os.getenv('MINIO_ACCESS_KEY', 'minioadmin')\n",
    "MINIO_SECRET_KEY = os.getenv('MINIO_SECRET_KEY', 'minioadmin')\n",
    "MINIO_BUCKET = os.getenv('MINIO_BUCKET', 'enderecos')\n",
    "\n",
    "# Configuração do Spark com DeltaLake\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"MotorCorrespondenciaEnderecos\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:2.4.0,org.apache.hadoop:hadoop-aws:3.3.2\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", f\"http://{MINIO_ENDPOINT}\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .master(\"local[*]\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# Configurar log level\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"Spark Session criada com sucesso!\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"MinIO Endpoint: {MINIO_ENDPOINT}\")\n",
    "print(f\"Bucket: {MINIO_BUCKET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir caminhos base no MinIO\n",
    "BASE_PATH = f\"s3a://{MINIO_BUCKET}\"\n",
    "\n",
    "# Caminhos para cada camada\n",
    "PATH_BRONZE = f\"{BASE_PATH}/bronze\"  # Dados brutos\n",
    "PATH_SILVER = f\"{BASE_PATH}/silver\"  # Dados normalizados (Camada Prata)\n",
    "PATH_GOLD = f\"{BASE_PATH}/gold\"       # Dados canônicos (Camada Ouro)\n",
    "\n",
    "# Caminhos específicos\n",
    "PATH_ENDERECOS_LIVRES = f\"{PATH_BRONZE}/enderecos_livres\"\n",
    "PATH_DNE = f\"{PATH_BRONZE}/dne\"\n",
    "PATH_CNEFE = f\"{PATH_BRONZE}/cnefe\"\n",
    "PATH_OSM = f\"{PATH_BRONZE}/osm\"\n",
    "\n",
    "PATH_ENDERECOS_ESTRUTURADOS = f\"{PATH_SILVER}/enderecos_estruturados\"\n",
    "PATH_ENDERECOS_NORMALIZADOS = f\"{PATH_SILVER}/enderecos_normalizados\"\n",
    "\n",
    "PATH_CAMADA_OURO = f\"{PATH_GOLD}/camada_ouro\"\n",
    "PATH_CLUSTERS = f\"{PATH_GOLD}/clusters\"\n",
    "PATH_MATCHES = f\"{PATH_GOLD}/matches\"\n",
    "\n",
    "print(\"Caminhos configurados:\")\n",
    "print(f\"Bronze: {PATH_BRONZE}\")\n",
    "print(f\"Silver: {PATH_SILVER}\")\n",
    "print(f\"Gold: {PATH_GOLD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função auxiliar para salvar DataFrame como Delta Table\n",
    "def save_delta_table(df, path, mode=\"overwrite\", partition_by=None):\n",
    "    \"\"\"\n",
    "    Salva um DataFrame como tabela Delta no MinIO\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame do Spark\n",
    "        path: Caminho no MinIO (s3a://)\n",
    "        mode: Modo de escrita (overwrite, append, errorIfExists)\n",
    "        partition_by: Lista de colunas para particionamento\n",
    "    \"\"\"\n",
    "    writer = df.write.format(\"delta\").mode(mode)\n",
    "    \n",
    "    if partition_by:\n",
    "        writer = writer.partitionBy(*partition_by)\n",
    "    \n",
    "    writer.save(path)\n",
    "    print(f\"Dados salvos em: {path}\")\n",
    "\n",
    "# Função auxiliar para ler Delta Table\n",
    "def read_delta_table(path):\n",
    "    \"\"\"\n",
    "    Lê uma tabela Delta do MinIO\n",
    "    \n",
    "    Args:\n",
    "        path: Caminho no MinIO (s3a://)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame do Spark\n",
    "    \"\"\"\n",
    "    return spark.read.format(\"delta\").load(path)\n",
    "\n",
    "print(\"Funções auxiliares definidas!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
