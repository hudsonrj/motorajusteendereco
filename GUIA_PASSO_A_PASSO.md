# Guia Passo a Passo - Motor de Correspond√™ncia de Endere√ßos

Este guia detalha todo o processo desde o carregamento dos dados at√© a execu√ß√£o completa do motor de correspond√™ncia.

## üìã √çndice

1. [Pr√©-requisitos](#pr√©-requisitos)
2. [Configura√ß√£o Inicial](#configura√ß√£o-inicial)
3. [Carregamento dos Dados (Camada Bronze)](#carregamento-dos-dados-camada-bronze)
4. [Tratamento Inicial com NER](#tratamento-inicial-com-ner)
5. [Normaliza√ß√£o (Camada Prata)](#normaliza√ß√£o-camada-prata)
6. [Cria√ß√£o da Camada Ouro](#cria√ß√£o-da-camada-ouro)
7. [Clusteriza√ß√£o](#clusteriza√ß√£o)
8. [Motor de Correspond√™ncia](#motor-de-correspond√™ncia)
9. [Valida√ß√£o Geogr√°fica](#valida√ß√£o-geogr√°fica)
10. [Verifica√ß√£o e An√°lise](#verifica√ß√£o-e-an√°lise)

---

## 1. Pr√©-requisitos

### 1.1 Infraestrutura Necess√°ria

- ‚úÖ MinIO configurado e rodando
- ‚úÖ Apache Spark configurado
- ‚úÖ DeltaLake instalado
- ‚úÖ Jupyter Notebook com PySpark
- ‚úÖ Credenciais de acesso ao MinIO configuradas

### 1.2 Vari√°veis de Ambiente

Configure as seguintes vari√°veis de ambiente antes de iniciar:

```bash
export MINIO_ENDPOINT="http://minio:9000"
export MINIO_ACCESS_KEY="seu_access_key"
export MINIO_SECRET_KEY="seu_secret_key"
export MINIO_BUCKET="enderecos"
```

### 1.3 Estrutura de Diret√≥rios no MinIO

O MinIO deve ter a seguinte estrutura de buckets/diret√≥rios:

```
enderecos/
‚îú‚îÄ‚îÄ bronze/
‚îÇ   ‚îú‚îÄ‚îÄ enderecos_livres/
‚îÇ   ‚îú‚îÄ‚îÄ enderecos_com_erros/
‚îÇ   ‚îú‚îÄ‚îÄ dne/
‚îÇ   ‚îú‚îÄ‚îÄ cnefe/
‚îÇ   ‚îú‚îÄ‚îÄ osm/
‚îÇ   ‚îú‚îÄ‚îÄ dados_geograficos/
‚îÇ   ‚îî‚îÄ‚îÄ variacoes_enderecos/
‚îú‚îÄ‚îÄ silver/
‚îÇ   ‚îú‚îÄ‚îÄ enderecos_estruturados/
‚îÇ   ‚îî‚îÄ‚îÄ enderecos_normalizados/
‚îî‚îÄ‚îÄ gold/
    ‚îú‚îÄ‚îÄ camada_ouro/
    ‚îú‚îÄ‚îÄ clusters/
    ‚îî‚îÄ‚îÄ matches/
```

---

## 2. Configura√ß√£o Inicial

### Passo 2.1: Abrir o Notebook de Configura√ß√£o

1. Abra o Jupyter Notebook
2. Navegue at√© o diret√≥rio do projeto
3. Abra o notebook: `00_configuracao_inicial.ipynb`

### Passo 2.2: Verificar Configura√ß√µes

Execute a primeira c√©lula para verificar as configura√ß√µes:

```python
import os

# Verificar vari√°veis de ambiente
print("MINIO_ENDPOINT:", os.getenv("MINIO_ENDPOINT", "N√£o configurado"))
print("MINIO_ACCESS_KEY:", os.getenv("MINIO_ACCESS_KEY", "N√£o configurado"))
print("MINIO_SECRET_KEY:", "***" if os.getenv("MINIO_SECRET_KEY") else "N√£o configurado")
```

**Sa√≠da esperada:**
```
MINIO_ENDPOINT: http://minio:9000
MINIO_ACCESS_KEY: seu_access_key
MINIO_SECRET_KEY: ***
```

### Passo 2.3: Executar Configura√ß√£o Completa

Execute todas as c√©lulas do notebook `00_configuracao_inicial.ipynb`:

1. **C√©lula 1**: Importa√ß√µes e verifica√ß√µes
2. **C√©lula 2**: Cria√ß√£o do SparkSession
3. **C√©lula 3**: Configura√ß√£o do DeltaLake e MinIO
4. **C√©lula 4**: Defini√ß√£o de caminhos
5. **C√©lula 5**: Fun√ß√µes utilit√°rias (`save_delta_table`, `read_delta_table`)

**Verifica√ß√£o:**
```python
# Verificar se SparkSession foi criado
print(f"Spark Version: {spark.version}")
print(f"DeltaLake Version: {spark.sql('SELECT spark.databricks.delta.optimize.maxFileSize').show()}")

# Verificar conex√£o com MinIO
try:
    # Tentar listar buckets
    print("‚úì Conex√£o com MinIO OK")
except Exception as e:
    print(f"‚úó Erro na conex√£o: {e}")
```

---

## 3. Carregamento dos Dados (Camada Bronze)

### Passo 3.1: Abrir Notebook de Carregamento

1. Abra o notebook: `00_carregar_dados_exemplo.ipynb`
2. Este notebook carrega todos os arquivos CSV para o MinIO

### Passo 3.2: Verificar Arquivos CSV Locais

Antes de carregar, verifique se os arquivos existem:

```python
import os

PATH_DADOS_LOCAL = "./dados-exemplo/bronze"
arquivos_esperados = [
    "enderecos_livres.csv",
    "enderecos_com_erros.csv",
    "dne.csv",
    "cnefe.csv",
    "osm.csv",
    "dados_geograficos.csv",
    "variacoes_enderecos.csv"
]

print("Verificando arquivos CSV...")
for arquivo in arquivos_esperados:
    caminho = f"{PATH_DADOS_LOCAL}/{arquivo}"
    existe = os.path.exists(caminho)
    status = "‚úì" if existe else "‚úó"
    print(f"{status} {arquivo}")

if all(os.path.exists(f"{PATH_DADOS_LOCAL}/{a}") for a in arquivos_esperados):
    print("\n‚úì Todos os arquivos encontrados!")
else:
    print("\n‚úó Alguns arquivos est√£o faltando. Execute gerar_dados_massa.py primeiro.")
```

### Passo 3.3: Executar Carregamento

Execute todas as c√©lulas do notebook `00_carregar_dados_exemplo.ipynb`:

**C√©lula 1**: Importar configura√ß√µes
```python
%run ./00_configuracao_inicial.ipynb
```

**C√©lula 2**: Definir arquivos e fun√ß√£o de carregamento
```python
# J√° est√° no notebook, apenas execute
```

**C√©lula 3**: Carregar endere√ßos livres
```python
df_enderecos_livres = carregar_csv_para_minio(
    "enderecos_livres",
    "enderecos_livres.csv",
    PATH_ENDERECOS_LIVRES
)
```

**Continue executando as c√©lulas para cada tipo de dado:**
- Endere√ßos com erros
- DNE
- CNEFE
- OSM
- Dados geogr√°ficos
- Varia√ß√µes

### Passo 3.4: Verificar Dados Carregados

Execute a c√©lula de resumo:

```python
# Resumo de dados carregados
tabelas = [
    ("Endere√ßos Livres", PATH_ENDERECOS_LIVRES),
    ("Endere√ßos com Erros", f"{PATH_BRONZE}/enderecos_com_erros"),
    ("DNE", PATH_DNE),
    ("CNEFE", PATH_CNEFE),
    ("OSM", PATH_OSM),
    ("Dados Geogr√°ficos", f"{PATH_BRONZE}/dados_geograficos"),
    ("Varia√ß√µes", f"{PATH_BRONZE}/variacoes_enderecos")
]

for nome, caminho in tabelas:
    try:
        df = read_delta_table(caminho)
        count = df.count()
        print(f"{nome:30} {count:>10} registros")
    except Exception as e:
        print(f"{nome:30} {'ERRO':>10} - {str(e)[:50]}")
```

**Sa√≠da esperada:**
```
Endere√ßos Livres                 1500 registros
Endere√ßos com Erros              1500 registros
DNE                              1200 registros
CNEFE                            1200 registros
OSM                              1200 registros
Dados Geogr√°ficos                1500 registros
Varia√ß√µes                        1500 registros
```

### Passo 3.5: Visualizar Amostra dos Dados

```python
# Visualizar endere√ßos livres
df_enderecos_livres = read_delta_table(PATH_ENDERECOS_LIVRES)
print("Amostra de Endere√ßos Livres:")
df_enderecos_livres.show(10, truncate=False)

# Visualizar DNE
df_dne = read_delta_table(PATH_DNE)
print("\nAmostra de DNE:")
df_dne.show(5, truncate=False)
```

---

## 4. Tratamento Inicial com NER

### Passo 4.1: Abrir Notebook de NER

1. Abra o notebook: `01_tratamento_inicial_ner.ipynb`
2. Este notebook estrutura endere√ßos livres usando regex

### Passo 4.2: Executar Configura√ß√£o

Execute a primeira c√©lula para importar configura√ß√µes:

```python
%run ./00_configuracao_inicial.ipynb
```

### Passo 4.3: Carregar Dados de Entrada

```python
# Carregar endere√ßos livres
df_enderecos_livres = read_delta_table(PATH_ENDERECOS_LIVRES)

print(f"Total de endere√ßos livres: {df_enderecos_livres.count()}")
df_enderecos_livres.show(5, truncate=False)
```

### Passo 4.4: Executar Estrutura√ß√£o

Execute a fun√ß√£o de estrutura√ß√£o:

```python
# Aplicar estrutura√ß√£o NER
df_estruturado = estruturar_endereco_livre(df_enderecos_livres)

# Visualizar resultado
print("Endere√ßos Estruturados:")
df_estruturado.show(10, truncate=False)
```

**Verificar estrutura:**
```python
# Verificar schema
df_estruturado.printSchema()

# Verificar campos esperados
campos_esperados = ["id", "endereco_livre", "tipo_logradouro", "nome_logradouro", 
                    "numero", "bairro", "complemento", "uf", "cidade"]
campos_presentes = df_estruturado.columns

for campo in campos_esperados:
    status = "‚úì" if campo in campos_presentes else "‚úó"
    print(f"{status} {campo}")
```

### Passo 4.5: Salvar na Camada Silver

```python
# Salvar endere√ßos estruturados
save_delta_table(
    df_estruturado,
    PATH_ENDERECOS_ESTRUTURADOS,
    mode="overwrite"
)

print("‚úì Endere√ßos estruturados salvos na camada Silver")
```

### Passo 4.6: An√°lise de Qualidade

```python
# Verificar qualidade da estrutura√ß√£o
from pyspark.sql.functions import col, when, count

df_qualidade = df_estruturado.select(
    count("*").alias("total"),
    count(when(col("tipo_logradouro").isNotNull(), 1)).alias("com_tipo"),
    count(when(col("nome_logradouro").isNotNull(), 1)).alias("com_nome"),
    count(when(col("numero").isNotNull(), 1)).alias("com_numero"),
    count(when(col("bairro").isNotNull(), 1)).alias("com_bairro")
)

df_qualidade.show()
```

**Sa√≠da esperada:**
```
+-----+--------+--------+----------+----------+
|total|com_tipo|com_nome|com_numero|com_bairro|
+-----+--------+--------+----------+----------+
| 1500|    1450|    1480|      1420|      1350|
+-----+--------+--------+----------+----------+
```

---

## 5. Normaliza√ß√£o (Camada Prata)

### Passo 5.1: Abrir Notebook de Normaliza√ß√£o

1. Abra o notebook: `02_normalizacao_camada_prata.ipynb`
2. Este notebook normaliza os endere√ßos estruturados

### Passo 5.2: Carregar Dados Estruturados

```python
%run ./00_configuracao_inicial.ipynb

# Carregar endere√ßos estruturados
df_estruturado = read_delta_table(PATH_ENDERECOS_ESTRUTURADOS)

print(f"Total de endere√ßos estruturados: {df_estruturado.count()}")
```

### Passo 5.3: Executar Normaliza√ß√£o

```python
# Aplicar normaliza√ß√£o
df_normalizado = normalizar_endereco(df_estruturado)

# Visualizar resultado
print("Endere√ßos Normalizados (antes vs depois):")
df_normalizado.select(
    "endereco_livre",
    "tipo_logradouro",
    "nome_logradouro_normalizado",
    "bairro_normalizado"
).show(10, truncate=False)
```

### Passo 5.4: Verificar Normaliza√ß√µes Aplicadas

```python
# Exemplos de normaliza√ß√µes
exemplos = df_normalizado.filter(
    (col("nome_logradouro") != col("nome_logradouro_normalizado")) |
    (col("tipo_logradouro") != col("tipo_logradouro_normalizado"))
).select(
    "tipo_logradouro",
    "tipo_logradouro_normalizado",
    "nome_logradouro",
    "nome_logradouro_normalizado"
).limit(20)

exemplos.show(truncate=False)
```

**Exemplos esperados:**
- "r." ‚Üí "RUA"
- "Av." ‚Üí "AVENIDA"
- "XV" ‚Üí "QUINZE"
- "souza" ‚Üí "SOUSA"

### Passo 5.5: Salvar Dados Normalizados

```python
# Salvar na camada Silver
save_delta_table(
    df_normalizado,
    PATH_ENDERECOS_NORMALIZADOS,
    mode="overwrite"
)

print("‚úì Endere√ßos normalizados salvos na camada Silver")
```

---

## 6. Cria√ß√£o da Camada Ouro

### Passo 6.1: Abrir Notebook da Camada Ouro

1. Abra o notebook: `03_camada_ouro_deduplicacao.ipynb`
2. Este notebook cria o "Golden Record" unificando fontes

### Passo 6.2: Carregar Todas as Fontes

```python
%run ./00_configuracao_inicial.ipynb

# Carregar todas as fontes de refer√™ncia
df_dne = read_delta_table(PATH_DNE)
df_cnefe = read_delta_table(PATH_CNEFE)
df_osm = read_delta_table(PATH_OSM)

print(f"DNE: {df_dne.count()} registros")
print(f"CNEFE: {df_cnefe.count()} registros")
print(f"OSM: {df_osm.count()} registros")
```

### Passo 6.3: Normalizar Fontes de Refer√™ncia

Antes de unificar, normalizar as fontes:

```python
# Normalizar DNE
df_dne_normalizado = normalizar_endereco(df_dne)

# Normalizar CNEFE
df_cnefe_normalizado = normalizar_endereco(df_cnefe)

# Normalizar OSM
df_osm_normalizado = normalizar_endereco(df_osm)
```

### Passo 6.4: Unificar Fontes

```python
# Carregar fontes com metadados
df_dne_carregado = carregar_fonte(df_dne_normalizado, "DNE")
df_cnefe_carregado = carregar_fonte(df_cnefe_normalizado, "CNEFE")
df_osm_carregado = carregar_fonte(df_osm_normalizado, "OSM")

# Unificar
df_unificado = unificar_fontes([
    df_dne_carregado,
    df_cnefe_carregado,
    df_osm_carregado
])

print(f"Total de registros unificados: {df_unificado.count()}")
```

### Passo 6.5: Deduplicar

```python
# Deduplicar por hash
df_deduplicado = deduplicar_por_hash(df_unificado)

print(f"Registros ap√≥s deduplica√ß√£o: {df_deduplicado.count()}")
print(f"Duplicatas removidas: {df_unificado.count() - df_deduplicado.count()}")
```

### Passo 6.6: Criar Registros Can√¥nicos

```python
# Criar registros can√¥nicos
df_camada_ouro = criar_registro_canonico(df_deduplicado)

# Visualizar
print("Camada Ouro - Registros Can√¥nicos:")
df_camada_ouro.select(
    "uid",
    "tipo_logradouro",
    "nome_logradouro",
    "numero",
    "bairro",
    "cidade",
    "uf",
    "confianca_score",
    "fontes"
).show(20, truncate=False)
```

### Passo 6.7: Salvar Camada Ouro

```python
# Salvar na camada Gold
save_delta_table(
    df_camada_ouro,
    PATH_CAMADA_OURO,
    mode="overwrite"
)

print("‚úì Camada Ouro criada e salva!")
```

### Passo 6.8: Estat√≠sticas da Camada Ouro

```python
from pyspark.sql.functions import avg, min, max, count

estatisticas = df_camada_ouro.agg(
    count("*").alias("total_registros"),
    avg("confianca_score").alias("confianca_media"),
    min("confianca_score").alias("confianca_min"),
    max("confianca_score").alias("confianca_max"),
    count("fontes").alias("com_fontes")
)

estatisticas.show()
```

---

## 7. Clusteriza√ß√£o

### Passo 7.1: Abrir Notebook de Clusteriza√ß√£o

1. Abra o notebook: `04_clusterizacao.ipynb`
2. Este notebook agrupa varia√ß√µes do mesmo endere√ßo

### Passo 7.2: Carregar Camada Ouro

```python
%run ./00_configuracao_inicial.ipynb

# Carregar Camada Ouro
df_camada_ouro = read_delta_table(PATH_CAMADA_OURO)

print(f"Registros na Camada Ouro: {df_camada_ouro.count()}")
```

### Passo 7.3: Executar Clusteriza√ß√£o

```python
# Criar clusters
df_clusters = criar_clusters_simples(df_camada_ouro)

# Visualizar clusters
print("Clusters Criados:")
df_clusters.select(
    "cluster_id",
    "tipo_logradouro",
    "nome_logradouro",
    "numero",
    "bairro",
    "cidade",
    "uf"
).orderBy("cluster_id").show(30, truncate=False)
```

### Passo 7.4: Analisar Clusters

```python
# Estat√≠sticas de clusters
from pyspark.sql.functions import count

estatisticas_clusters = df_clusters.groupBy("cluster_id").agg(
    count("*").alias("tamanho_cluster")
).agg(
    count("*").alias("total_clusters"),
    avg("tamanho_cluster").alias("tamanho_medio"),
    min("tamanho_cluster").alias("tamanho_min"),
    max("tamanho_cluster").alias("tamanho_max")
)

estatisticas_clusters.show()
```

### Passo 7.5: Salvar Clusters

```python
# Salvar clusters
save_delta_table(
    df_clusters,
    PATH_CLUSTERS,
    mode="overwrite"
)

print("‚úì Clusters salvos!")
```

---

## 8. Motor de Correspond√™ncia

### Passo 8.1: Abrir Notebook do Motor

1. Abra o notebook: `05_motor_correspondencia.ipynb`
2. Este notebook encontra matches entre endere√ßos de entrada e Camada Ouro

### Passo 8.2: Carregar Dados

```python
%run ./00_configuracao_inicial.ipynb

# Carregar endere√ßos normalizados (entrada)
df_entrada = read_delta_table(PATH_ENDERECOS_NORMALIZADOS)

# Carregar Camada Ouro
df_camada_ouro = read_delta_table(PATH_CAMADA_OURO)

print(f"Endere√ßos de entrada: {df_entrada.count()}")
print(f"Registros na Camada Ouro: {df_camada_ouro.count()}")
```

### Passo 8.3: Aplicar Blocking R√≠gido

```python
# Blocking por UF + Cidade
df_candidatos_rigido = aplicar_blocking_rigido(df_entrada, df_camada_ouro)

print(f"Candidatos ap√≥s blocking r√≠gido: {df_candidatos_rigido.count()}")
df_candidatos_rigido.show(10, truncate=False)
```

### Passo 8.4: Aplicar Blocking Flex√≠vel

```python
# Blocking por SoundexBR
df_candidatos_flexivel = aplicar_blocking_flexivel(df_entrada, df_camada_ouro)

print(f"Candidatos ap√≥s blocking flex√≠vel: {df_candidatos_flexivel.count()}")
```

### Passo 8.5: Calcular Similaridades

```python
# Calcular scores de similaridade
df_matches = calcular_scores_similaridade(df_candidatos_rigido)

# Visualizar matches
print("Top 20 Matches:")
df_matches.select(
    "id_entrada",
    "endereco_livre",
    "uid_ouro",
    "nome_logradouro_ouro",
    "score_jaro_winkler",
    "score_levenshtein",
    "score_final"
).orderBy(col("score_final").desc()).show(20, truncate=False)
```

### Passo 8.6: Filtrar Matches por Threshold

```python
# Filtrar matches com score m√≠nimo
THRESHOLD = 0.7

df_matches_filtrados = df_matches.filter(col("score_final") >= THRESHOLD)

print(f"Matches acima do threshold ({THRESHOLD}): {df_matches_filtrados.count()}")
```

### Passo 8.7: Executar Motor Completo

```python
# Executar motor completo
df_resultado = executar_motor_correspondencia(
    df_entrada,
    df_camada_ouro,
    threshold=0.7
)

print(f"Total de matches encontrados: {df_resultado.count()}")
```

### Passo 8.8: Salvar Resultados

```python
# Salvar matches
save_delta_table(
    df_resultado,
    PATH_MATCHES,
    mode="overwrite"
)

print("‚úì Matches salvos!")
```

---

## 9. Valida√ß√£o Geogr√°fica

### Passo 9.1: Abrir Notebook de Valida√ß√£o

1. Abra o notebook: `06_validacao_geografica.ipynb`
2. Este notebook valida matches usando coordenadas geogr√°ficas

### Passo 9.2: Carregar Dados

```python
%run ./00_configuracao_inicial.ipynb

# Carregar matches
df_matches = read_delta_table(PATH_MATCHES)

# Carregar dados geogr√°ficos
df_geograficos = read_delta_table(f"{PATH_BRONZE}/dados_geograficos")

# Carregar Camada Ouro (com coordenadas)
df_camada_ouro = read_delta_table(PATH_CAMADA_OURO)

print(f"Matches para validar: {df_matches.count()}")
```

### Passo 9.3: Validar Correspond√™ncia Geogr√°fica

```python
# Validar correspond√™ncia geogr√°fica
df_validado = validar_correspondencia_geografica(
    df_matches,
    df_camada_ouro,
    df_geograficos
)

print("Matches Validados:")
df_validado.select(
    "id_entrada",
    "uid_ouro",
    "score_final",
    "distancia_metros",
    "validacao_geografica",
    "score_final_validado"
).orderBy(col("score_final_validado").desc()).show(20, truncate=False)
```

### Passo 9.4: Executar Valida√ß√£o Completa

```python
# Executar valida√ß√£o completa
df_resultado_final = executar_validacao_geografica_completa(
    df_matches,
    df_camada_ouro,
    df_geograficos
)

print(f"Matches validados: {df_resultado_final.count()}")
```

### Passo 9.5: An√°lise de Valida√ß√£o

```python
# Estat√≠sticas de valida√ß√£o
from pyspark.sql.functions import count, when, avg

estatisticas_validacao = df_resultado_final.agg(
    count("*").alias("total_matches"),
    count(when(col("validacao_geografica") == "ALTA", 1)).alias("validacao_alta"),
    count(when(col("validacao_geografica") == "MEDIA", 1)).alias("validacao_media"),
    count(when(col("validacao_geografica") == "BAIXA", 1)).alias("validacao_baixa"),
    avg("distancia_metros").alias("distancia_media_metros"),
    avg("score_final_validado").alias("score_medio_final")
)

estatisticas_validacao.show()
```

### Passo 9.6: Salvar Resultados Finais

```python
# Salvar resultados finais
save_delta_table(
    df_resultado_final,
    f"{PATH_GOLD}/matches_validados",
    mode="overwrite"
)

print("‚úì Valida√ß√£o geogr√°fica conclu√≠da e resultados salvos!")
```

---

## 10. Verifica√ß√£o e An√°lise

### Passo 10.1: Criar Notebook de An√°lise

Crie um novo notebook `07_analise_resultados.ipynb` para an√°lise final:

```python
%run ./00_configuracao_inicial.ipynb

# Carregar resultados finais
df_resultado_final = read_delta_table(f"{PATH_GOLD}/matches_validados")

print("="*60)
print("AN√ÅLISE FINAL - MOTOR DE CORRESPOND√äNCIA")
print("="*60)
```

### Passo 10.2: Estat√≠sticas Gerais

```python
from pyspark.sql.functions import *

# Estat√≠sticas gerais
estatisticas_gerais = df_resultado_final.agg(
    count("*").alias("total_matches"),
    count(when(col("score_final_validado") >= 0.9, 1)).alias("matches_alta_confianca"),
    count(when(col("score_final_validado") >= 0.7, 1)).alias("matches_media_confianca"),
    count(when(col("score_final_validado") < 0.7, 1)).alias("matches_baixa_confianca"),
    avg("score_final_validado").alias("score_medio"),
    min("score_final_validado").alias("score_minimo"),
    max("score_final_validado").alias("score_maximo")
)

estatisticas_gerais.show()
```

### Passo 10.3: An√°lise por Cidade

```python
# An√°lise por cidade
analise_cidade = df_resultado_final.groupBy("cidade_entrada", "uf_entrada").agg(
    count("*").alias("total_matches"),
    avg("score_final_validado").alias("score_medio"),
    count(when(col("score_final_validado") >= 0.9, 1)).alias("alta_confianca")
).orderBy(col("total_matches").desc())

analise_cidade.show()
```

### Passo 10.4: Top Matches

```python
# Top 50 matches com maior score
top_matches = df_resultado_final.select(
    "id_entrada",
    "endereco_livre",
    "nome_logradouro_ouro",
    "numero_ouro",
    "bairro_ouro",
    "cidade_ouro",
    "score_final_validado",
    "distancia_metros",
    "validacao_geografica"
).orderBy(col("score_final_validado").desc()).limit(50)

top_matches.show(50, truncate=False)
```

### Passo 10.5: Casos sem Match

```python
# Carregar endere√ßos de entrada
df_entrada = read_delta_table(PATH_ENDERECOS_NORMALIZADOS)

# Encontrar endere√ßos sem match
ids_com_match = df_resultado_final.select("id_entrada").distinct()
df_sem_match = df_entrada.join(
    ids_com_match,
    df_entrada.id == ids_com_match.id_entrada,
    "left_anti"
)

print(f"Endere√ßos sem match: {df_sem_match.count()}")
print("\nExemplos de endere√ßos sem match:")
df_sem_match.select("id", "endereco_livre", "cidade", "uf").show(20, truncate=False)
```

### Passo 10.6: Exportar Resultados

```python
# Exportar resultados para CSV (opcional)
df_resultado_final.coalesce(1).write.mode("overwrite").option("header", "true").csv(
    "s3a://enderecos/resultados/matches_finais"
)

print("‚úì Resultados exportados!")
```

---

## üîç Troubleshooting

### Problema: Erro ao conectar com MinIO

**Solu√ß√£o:**
```python
# Verificar vari√°veis de ambiente
import os
print(os.getenv("MINIO_ENDPOINT"))
print(os.getenv("MINIO_ACCESS_KEY"))

# Verificar configura√ß√£o do Spark
spark.conf.get("spark.hadoop.fs.s3a.endpoint")
spark.conf.get("spark.hadoop.fs.s3a.access.key")
```

### Problema: Erro ao ler Delta Table

**Solu√ß√£o:**
```python
# Verificar se a tabela existe
try:
    df = spark.read.format("delta").load(PATH_ENDERECOS_LIVRES)
    print(f"‚úì Tabela existe: {df.count()} registros")
except Exception as e:
    print(f"‚úó Erro: {e}")
    print("Verifique se os dados foram carregados corretamente.")
```

### Problema: Performance lenta

**Solu√ß√µes:**
1. Aumentar n√∫mero de parti√ß√µes:
```python
df = df.repartition(200)
```

2. Cachear DataFrames frequentemente usados:
```python
df_camada_ouro.cache()
```

3. Usar broadcast join para tabelas pequenas:
```python
from pyspark.sql.functions import broadcast
df_resultado = df_entrada.join(broadcast(df_camada_ouro), ...)
```

---

## üìä Checklist de Execu√ß√£o

Use este checklist para garantir que todos os passos foram executados:

- [ ] Configura√ß√£o inicial conclu√≠da
- [ ] Dados CSV carregados para MinIO (Bronze)
- [ ] Endere√ßos estruturados (Silver)
- [ ] Endere√ßos normalizados (Silver)
- [ ] Camada Ouro criada (Gold)
- [ ] Clusters criados
- [ ] Motor de correspond√™ncia executado
- [ ] Valida√ß√£o geogr√°fica conclu√≠da
- [ ] An√°lise de resultados realizada
- [ ] Resultados exportados

---

## üìù Notas Finais

1. **Ordem de Execu√ß√£o**: Execute os notebooks na ordem num√©rica (00 ‚Üí 01 ‚Üí 02 ‚Üí ...)
2. **Depend√™ncias**: Cada notebook depende dos anteriores
3. **Modo Overwrite**: Os notebooks usam `mode="overwrite"` - cuidado em produ√ß√£o
4. **Performance**: Para grandes volumes, considere particionar por cidade/UF
5. **Monitoramento**: Acompanhe logs do Spark para identificar gargalos

---

## üöÄ Pr√≥ximos Passos

1. **Otimiza√ß√£o**: Ajustar thresholds e par√¢metros de similaridade
2. **Valida√ß√£o**: Comparar resultados com dados conhecidos
3. **Produ√ß√£o**: Adaptar para processamento incremental
4. **API**: Criar API REST para consultas em tempo real
5. **Monitoramento**: Implementar m√©tricas e alertas

---

**Fim do Guia Passo a Passo**

Para d√∫vidas ou problemas, consulte os notebooks individuais ou a documenta√ß√£o do projeto.
